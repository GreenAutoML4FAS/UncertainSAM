{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Speed Test",
   "id": "54a93aee73f7c506"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from os.path import join, dirname\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "import usam\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "from usam.patch_sam2 import patch_sam2\n",
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table, parameter_count\n",
    "\n",
    "\n",
    "root = dirname(usam.__path__[0])\n",
    "model_cfg = join(root, \"models\", \"sam\", \"configs_2.0\", \"sam2_hiera_l.yaml\")\n",
    "checkpoint = join(root, \"models\", \"sam\", \"checkpoints_2.0\", \"sam2_hiera_large.pt\")\n",
    "#model_cfg = join(root, \"models\", \"sam\", \"configs_2.0\", \"sam2_hiera_b+.yaml\")\n",
    "#checkpoint = join(root, \"models\", \"sam\", \"checkpoints_2.0\", \"sam2_hiera_base_plus.pt\")\n",
    "#model_cfg = join(root, \"models\", \"sam\", \"configs_2.0\", \"sam2_hiera_s.yaml\")\n",
    "#checkpoint = join(root, \"models\", \"sam\", \"checkpoints_2.0\", \"sam2_hiera_small.pt\")\n",
    "#model_cfg = join(root, \"models\", \"sam\", \"configs_2.0\", \"sam2_hiera_t.yaml\")\n",
    "#checkpoint = join(root, \"models\", \"sam\", \"checkpoints_2.0\", \"sam2_hiera_tiny.pt\")\n",
    "mlp_dir = join(root, \"models\", \"mlps\", \"sam2.0\")\n",
    "\n",
    "\n",
    "image = Image.open(join(root, \"assets\", \"dog_sample.png\"))\n",
    "image = np.array(image.convert(\"RGB\"))\n",
    "input_point = np.array([[256, 256]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "\n",
    "num_iterations = 50\n",
    "    \n",
    "    \n",
    "# Load predictor\n",
    "predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\n",
    "print(parameter_count_table(predictor.model))\n",
    "print(\"Parameters: \", parameter_count(predictor.model))\n",
    "\n",
    "# Do some warmups to make the GPU ready\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    for _ in range(num_iterations):\n",
    "        predictor.set_image(image)\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=False,\n",
    "        )"
   ],
   "id": "a69afa8354b95a6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speed Test for MC-based image augmented SAM2",
   "id": "4e5053bf2222356c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_img_augmentations = 5\n",
    "\n",
    "# Run one iteration for warmup\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    for _ in range(num_iterations):\n",
    "        for _ in range(num_img_augmentations):\n",
    "            aug_image = cv2.flip(image, 1)\n",
    "            predictor.set_image(aug_image)\n",
    "            masks, scores, logits = predictor.predict(\n",
    "                point_coords=input_point,\n",
    "                point_labels=input_label,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "    \n",
    "# Run speed test\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    t0 = datetime.now()\n",
    "    for _ in range(num_iterations):\n",
    "        for _ in range(num_img_augmentations):\n",
    "            aug_image = cv2.flip(image, 1)\n",
    "            predictor.set_image(aug_image)\n",
    "            masks, scores, logits = predictor.predict(\n",
    "                point_coords=input_point,\n",
    "                point_labels=input_label,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "    t1 = datetime.now()\n",
    "    t_image_aug = (t1 - t0).total_seconds() / num_iterations\n",
    "    print(f\"MC-simulation with image augmentation in SAM2: {(t1 - t0).total_seconds() / num_iterations:.5f} s per iteration\")"
   ],
   "id": "c1eab96f059b989e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speed Test for MC-based prompt augmented SAM2",
   "id": "5df1726949317044"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_prompt_augmentations = 8\n",
    "\n",
    "# Run one iteration for warmup\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image(image)\n",
    "    for _ in range(num_prompt_augmentations):\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "    \n",
    "# Run speed test\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    t0 = datetime.now()\n",
    "    for _ in range(num_iterations):\n",
    "        predictor.set_image(image)\n",
    "        for _ in range(num_prompt_augmentations):\n",
    "            masks, scores, logits = predictor.predict(\n",
    "                point_coords=input_point,\n",
    "                point_labels=input_label,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "    t1 = datetime.now()\n",
    "    t_prompt_aug = (t1 - t0).total_seconds() / num_iterations\n",
    "    print(f\"MC-simulation with prompt augmentation in SAM2: {(t1 - t0).total_seconds() / num_iterations:.5f} s per iteration\")"
   ],
   "id": "ec03ca43ed319f06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speed Test for standard SAM2 with entropy calculation",
   "id": "ede3bccad542c458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def entropy(mask):\n",
    "    mask = np.clip(mask, 1e-6, 1 - 1e-6)\n",
    "    e = -mask * np.log2(mask) - (1 - mask) * np.log2(1 - mask)\n",
    "    e = mask * e\n",
    "    e = np.sum(e) / np.sum(mask)\n",
    "    return e\n",
    "\n",
    "# Run one iteration for warmup\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image(image)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    e = entropy(masks[0])\n",
    "\n",
    "# Run speed test\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    t0 = datetime.now()\n",
    "    for _ in range(num_iterations):\n",
    "        predictor.set_image(image)\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        e = entropy(masks[0])\n",
    "    t1 = datetime.now()\n",
    "    t_entropy = (t1 - t0).total_seconds() / num_iterations\n",
    "    print(f\"Entropy of SAM2: {(t1 - t0).total_seconds() / num_iterations:.5f} s per iteration\")"
   ],
   "id": "f9b28e862c171a6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speed Test for normal SAM2 ",
   "id": "44857645f916d7c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Run one iteration for warmup\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image(image)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "# Run speed test\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    t0 = datetime.now()\n",
    "    for _ in range(num_iterations):\n",
    "        predictor.set_image(image)\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "    t1 = datetime.now()\n",
    "    t_standard = (t1 - t0).total_seconds() / num_iterations\n",
    "    print(f\"Normal SAM2: {(t1 - t0).total_seconds() / num_iterations:.5f} s per iteration\")\n",
    "    "
   ],
   "id": "adff0b50ad51a870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speed Test for USAM extended SAM2",
   "id": "51bdd5c2c400feea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Patch SAM2 with USAM extension\n",
    "patch_sam2(predictor, mlp_dir)\n",
    "\n",
    "# Print MLP Parameters\n",
    "for name, mlp in predictor.model.sam_mask_decoder.regression_models.items():\n",
    "    print(f\"MLP {name}: {parameter_count_table(mlp)}\") \n",
    "    print(\"Parameters: \", parameter_count(mlp))\n",
    "\n",
    "# Run one iteration for warmup\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    predictor.set_image(image)\n",
    "    masks, scores, logits, mlp_scores = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    num_mlps = len(mlp_scores)\n",
    "    print(\"There are {} MLPs.\".format(len(mlp_scores)))\n",
    "    \n",
    "# Run speed test\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    t0 = datetime.now()\n",
    "    for _ in range(num_iterations):\n",
    "        predictor.set_image(image)\n",
    "        masks, scores, logits, mlp_scores = predictor.predict(\n",
    "            point_coords=input_point,\n",
    "            point_labels=input_label,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "    t1 = datetime.now()\n",
    "    t_usam = (t1 - t0).total_seconds() / num_iterations\n",
    "    print(f\"USAM extension in SAM2: {(t1 - t0).total_seconds() / num_iterations:.5f} s per iteration\")"
   ],
   "id": "656a049a6555a37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summarize results",
   "id": "6f8f58fdf7deb970"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"SAM2: {t_standard:.5f} s per iteration\")\n",
    "print(f\"SAM2 with Entropy: {t_entropy:.5f} s per iteration\")\n",
    "print(f\"SAM2 with {num_img_augmentations} MC image augmentations: {t_image_aug:.5f} s per iteration\")\n",
    "print(f\"SAM2 with {num_prompt_augmentations} MC prompt augmentations: {t_prompt_aug:.5f} s per iteration\")\n",
    "print(f\"SAM2 with all {num_mlps} (!) MLPs proposed in USAM: {t_usam:.5f} s per iteration\")\n"
   ],
   "id": "c985b773dcc1f1f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
